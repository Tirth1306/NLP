{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [Root]",
      "language": "python",
      "name": "Python [Root]"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Exploring NLTK.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J10I7v1j1jh"
      },
      "source": [
        "## **18BCE245 Tirth Patel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELhEeY5Ddsqg"
      },
      "source": [
        "# Exploring NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Whqm1LOdsqm"
      },
      "source": [
        "# NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzsNKsg_dsqn"
      },
      "source": [
        "NLP is a field of computer science and linguistic. It's an intersection between computer and human languages. Common task is natural language understanding. For more about Natural Language Processing visit <a href=\"https://en.wikipedia.org/wiki/Natural_language_processing\"> NLP</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Gtq4tcdsqn"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl1i085Odsqo"
      },
      "source": [
        "We can not use whole corpus as an input for parsing or text analysis. We have to break it down into something called tokens. Tokenization is a process of breaking stream of text into words, phrases or other meaningful elements. Most of the time tokenization is the first process in text analysis or NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdxCji6cdsqo"
      },
      "source": [
        "NLTK ( Natural Language Tool Kit for Python ) provides two types of functions for tokenization- sentence tokenization and word tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARQgPl-gdsqo"
      },
      "source": [
        "Sentence Tokenization - We break down the stream of text into a set of sentences. In this case our tokens would be sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSEirfx-dsqo"
      },
      "source": [
        "Word Tokenization - We break down the stream of text into a set of words. In this case our tokens would be words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9ODdzHRdsqp",
        "outputId": "254cc5c0-fa9a-4c82-e127-2377aecc32b3"
      },
      "source": [
        "#import our dependency -nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#import required modules\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "#uncomment below line if you haven't downloaded all nltk libs and corpus\n",
        "#nltk.download()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5Ke6nSydsqp",
        "outputId": "0e77944e-b11d-4088-d1ac-169fac188ad7"
      },
      "source": [
        "#get text\n",
        "corpus = open('MyText.txt','r').read()\n",
        "\n",
        "#here, f is a string or stream of text. Let's check it\n",
        "type(corpus)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMayEZYWdsqq"
      },
      "source": [
        "Now, we have our text, let's perform tokenization on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h9EnT8Idsqq",
        "outputId": "34963875-2ffe-4fd6-f3b7-dab73b91f6c6"
      },
      "source": [
        "#sentence tokenization\n",
        "sentences = sent_tokenize(corpus)\n",
        "sentences\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This chapter is divided into sections that skip between two quite different styles.',\n",
              " 'In the \"computing with language\" sections we will take on some linguistically motivated programming tasks without necessarily explaining how they work.',\n",
              " 'In the \"closer look at Python\" sections we will systematically review key programming concepts.',\n",
              " \"We'll flag the two styles in the section titles, but later chapters will mix both styles without being so up-front about it.\",\n",
              " 'We hope this style of introduction gives you an authentic taste of what will come later, while covering a range of elementary concepts in linguistics and computer science.',\n",
              " 'If you have basic familiarity with both areas, you can skip to 5; we will repeat any important points in later chapters, and if you miss anything you can easily consult the online reference material at http://nltk.org/.',\n",
              " 'If the material is completely new to you, this chapter will raise more questions than it answers, questions that are addressed in the rest of this book.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7VrWmw9dsqr",
        "outputId": "9da9f974-1005-4353-a17b-b8e220db9117"
      },
      "source": [
        "#word tokenization\n",
        "words = word_tokenize(corpus)\n",
        "words"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'chapter',\n",
              " 'is',\n",
              " 'divided',\n",
              " 'into',\n",
              " 'sections',\n",
              " 'that',\n",
              " 'skip',\n",
              " 'between',\n",
              " 'two',\n",
              " 'quite',\n",
              " 'different',\n",
              " 'styles',\n",
              " '.',\n",
              " 'In',\n",
              " 'the',\n",
              " '``',\n",
              " 'computing',\n",
              " 'with',\n",
              " 'language',\n",
              " \"''\",\n",
              " 'sections',\n",
              " 'we',\n",
              " 'will',\n",
              " 'take',\n",
              " 'on',\n",
              " 'some',\n",
              " 'linguistically',\n",
              " 'motivated',\n",
              " 'programming',\n",
              " 'tasks',\n",
              " 'without',\n",
              " 'necessarily',\n",
              " 'explaining',\n",
              " 'how',\n",
              " 'they',\n",
              " 'work',\n",
              " '.',\n",
              " 'In',\n",
              " 'the',\n",
              " '``',\n",
              " 'closer',\n",
              " 'look',\n",
              " 'at',\n",
              " 'Python',\n",
              " \"''\",\n",
              " 'sections',\n",
              " 'we',\n",
              " 'will',\n",
              " 'systematically',\n",
              " 'review',\n",
              " 'key',\n",
              " 'programming',\n",
              " 'concepts',\n",
              " '.',\n",
              " 'We',\n",
              " \"'ll\",\n",
              " 'flag',\n",
              " 'the',\n",
              " 'two',\n",
              " 'styles',\n",
              " 'in',\n",
              " 'the',\n",
              " 'section',\n",
              " 'titles',\n",
              " ',',\n",
              " 'but',\n",
              " 'later',\n",
              " 'chapters',\n",
              " 'will',\n",
              " 'mix',\n",
              " 'both',\n",
              " 'styles',\n",
              " 'without',\n",
              " 'being',\n",
              " 'so',\n",
              " 'up-front',\n",
              " 'about',\n",
              " 'it',\n",
              " '.',\n",
              " 'We',\n",
              " 'hope',\n",
              " 'this',\n",
              " 'style',\n",
              " 'of',\n",
              " 'introduction',\n",
              " 'gives',\n",
              " 'you',\n",
              " 'an',\n",
              " 'authentic',\n",
              " 'taste',\n",
              " 'of',\n",
              " 'what',\n",
              " 'will',\n",
              " 'come',\n",
              " 'later',\n",
              " ',',\n",
              " 'while',\n",
              " 'covering',\n",
              " 'a',\n",
              " 'range',\n",
              " 'of',\n",
              " 'elementary',\n",
              " 'concepts',\n",
              " 'in',\n",
              " 'linguistics',\n",
              " 'and',\n",
              " 'computer',\n",
              " 'science',\n",
              " '.',\n",
              " 'If',\n",
              " 'you',\n",
              " 'have',\n",
              " 'basic',\n",
              " 'familiarity',\n",
              " 'with',\n",
              " 'both',\n",
              " 'areas',\n",
              " ',',\n",
              " 'you',\n",
              " 'can',\n",
              " 'skip',\n",
              " 'to',\n",
              " '5',\n",
              " ';',\n",
              " 'we',\n",
              " 'will',\n",
              " 'repeat',\n",
              " 'any',\n",
              " 'important',\n",
              " 'points',\n",
              " 'in',\n",
              " 'later',\n",
              " 'chapters',\n",
              " ',',\n",
              " 'and',\n",
              " 'if',\n",
              " 'you',\n",
              " 'miss',\n",
              " 'anything',\n",
              " 'you',\n",
              " 'can',\n",
              " 'easily',\n",
              " 'consult',\n",
              " 'the',\n",
              " 'online',\n",
              " 'reference',\n",
              " 'material',\n",
              " 'at',\n",
              " 'http',\n",
              " ':',\n",
              " '//nltk.org/',\n",
              " '.',\n",
              " 'If',\n",
              " 'the',\n",
              " 'material',\n",
              " 'is',\n",
              " 'completely',\n",
              " 'new',\n",
              " 'to',\n",
              " 'you',\n",
              " ',',\n",
              " 'this',\n",
              " 'chapter',\n",
              " 'will',\n",
              " 'raise',\n",
              " 'more',\n",
              " 'questions',\n",
              " 'than',\n",
              " 'it',\n",
              " 'answers',\n",
              " ',',\n",
              " 'questions',\n",
              " 'that',\n",
              " 'are',\n",
              " 'addressed',\n",
              " 'in',\n",
              " 'the',\n",
              " 'rest',\n",
              " 'of',\n",
              " 'this',\n",
              " 'book',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2eZuhAMdsqr"
      },
      "source": [
        "# Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROM-2I0ndsqr"
      },
      "source": [
        "Stop words are most common words such as I, my, for etc.. in a particular language which doesn't affect our analytical process. So, we should remove these stop words from our corpus and focus on important words. \n",
        "\n",
        "NLTK provides a list of stopwords which we can use to remove stopwords from our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITMDhkGHdsqs",
        "outputId": "e29a7715-75c6-4483-b5af-d3de5cdc72d2"
      },
      "source": [
        "#import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d-oWRscdsqs",
        "outputId": "93e99688-eaf6-444a-8758-7ecd2c635114"
      },
      "source": [
        "s_words = stopwords.words('english')\n",
        "#let's have a look at few of them\n",
        "stopwords.words('english')[0:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBnqHcFNdsqs"
      },
      "source": [
        "Remove stopwords from a corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i11gJtIDdsqt",
        "outputId": "570128b8-4c5b-445b-93e8-8370c6b8b4ea"
      },
      "source": [
        "#we will use word tokenized text for further processing.\n",
        "filtered_corpus = [w for w in words if not w in s_words]\n",
        "\n",
        "filtered_corpus"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'chapter',\n",
              " 'divided',\n",
              " 'sections',\n",
              " 'skip',\n",
              " 'two',\n",
              " 'quite',\n",
              " 'different',\n",
              " 'styles',\n",
              " '.',\n",
              " 'In',\n",
              " '``',\n",
              " 'computing',\n",
              " 'language',\n",
              " \"''\",\n",
              " 'sections',\n",
              " 'take',\n",
              " 'linguistically',\n",
              " 'motivated',\n",
              " 'programming',\n",
              " 'tasks',\n",
              " 'without',\n",
              " 'necessarily',\n",
              " 'explaining',\n",
              " 'work',\n",
              " '.',\n",
              " 'In',\n",
              " '``',\n",
              " 'closer',\n",
              " 'look',\n",
              " 'Python',\n",
              " \"''\",\n",
              " 'sections',\n",
              " 'systematically',\n",
              " 'review',\n",
              " 'key',\n",
              " 'programming',\n",
              " 'concepts',\n",
              " '.',\n",
              " 'We',\n",
              " \"'ll\",\n",
              " 'flag',\n",
              " 'two',\n",
              " 'styles',\n",
              " 'section',\n",
              " 'titles',\n",
              " ',',\n",
              " 'later',\n",
              " 'chapters',\n",
              " 'mix',\n",
              " 'styles',\n",
              " 'without',\n",
              " 'up-front',\n",
              " '.',\n",
              " 'We',\n",
              " 'hope',\n",
              " 'style',\n",
              " 'introduction',\n",
              " 'gives',\n",
              " 'authentic',\n",
              " 'taste',\n",
              " 'come',\n",
              " 'later',\n",
              " ',',\n",
              " 'covering',\n",
              " 'range',\n",
              " 'elementary',\n",
              " 'concepts',\n",
              " 'linguistics',\n",
              " 'computer',\n",
              " 'science',\n",
              " '.',\n",
              " 'If',\n",
              " 'basic',\n",
              " 'familiarity',\n",
              " 'areas',\n",
              " ',',\n",
              " 'skip',\n",
              " '5',\n",
              " ';',\n",
              " 'repeat',\n",
              " 'important',\n",
              " 'points',\n",
              " 'later',\n",
              " 'chapters',\n",
              " ',',\n",
              " 'miss',\n",
              " 'anything',\n",
              " 'easily',\n",
              " 'consult',\n",
              " 'online',\n",
              " 'reference',\n",
              " 'material',\n",
              " 'http',\n",
              " ':',\n",
              " '//nltk.org/',\n",
              " '.',\n",
              " 'If',\n",
              " 'material',\n",
              " 'completely',\n",
              " 'new',\n",
              " ',',\n",
              " 'chapter',\n",
              " 'raise',\n",
              " 'questions',\n",
              " 'answers',\n",
              " ',',\n",
              " 'questions',\n",
              " 'addressed',\n",
              " 'rest',\n",
              " 'book',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJT08q8Rdsqt"
      },
      "source": [
        "Let's make sure that our new filtered corpus is really filtered.\n",
        "Most of the corpus even if it's a one sentence contains few stopwords.\n",
        "Obviously, lenth of the filtered text should be less than old corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR5--GlNdsqt",
        "outputId": "0de23801-d80f-4f82-d4b0-abdcabe7adb1"
      },
      "source": [
        "len(filtered_corpus) < len(words)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS9kvReidsqt",
        "outputId": "1b5fcbda-918a-4ab7-efa6-7a2beb299b46"
      },
      "source": [
        "#How much words did we remove?\n",
        "print(len(words) - len(filtered_corpus))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc-_ooHudsqu"
      },
      "source": [
        "# POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YD0wiqxdsqu"
      },
      "source": [
        "POS Tagging (Part of Speech Taggin) is a process of classification in which we classify words into a lexical category such as noun, verb.\n",
        "Apparently, it's very useful for future analytical processes such as structure analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nW61o7qdsqu",
        "outputId": "190e16f1-e81f-4c54-cf7c-93b709cc5424"
      },
      "source": [
        "#Here, we are going to use pos tagger provided by nltk, we can train our own pos_tag classifier as well\n",
        "#import\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#assign tags\n",
        "tags = pos_tag(filtered_corpus)\n",
        "tags"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This', 'DT'),\n",
              " ('chapter', 'NN'),\n",
              " ('divided', 'VBD'),\n",
              " ('sections', 'NNS'),\n",
              " ('skip', 'VBP'),\n",
              " ('two', 'CD'),\n",
              " ('quite', 'RB'),\n",
              " ('different', 'JJ'),\n",
              " ('styles', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('In', 'IN'),\n",
              " ('``', '``'),\n",
              " ('computing', 'JJ'),\n",
              " ('language', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " ('sections', 'NNS'),\n",
              " ('take', 'VBP'),\n",
              " ('linguistically', 'RB'),\n",
              " ('motivated', 'VBN'),\n",
              " ('programming', 'NN'),\n",
              " ('tasks', 'NNS'),\n",
              " ('without', 'IN'),\n",
              " ('necessarily', 'RB'),\n",
              " ('explaining', 'VBG'),\n",
              " ('work', 'NN'),\n",
              " ('.', '.'),\n",
              " ('In', 'IN'),\n",
              " ('``', '``'),\n",
              " ('closer', 'JJ'),\n",
              " ('look', 'NN'),\n",
              " ('Python', 'NNP'),\n",
              " (\"''\", \"''\"),\n",
              " ('sections', 'NNS'),\n",
              " ('systematically', 'RB'),\n",
              " ('review', 'VBP'),\n",
              " ('key', 'JJ'),\n",
              " ('programming', 'NN'),\n",
              " ('concepts', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('We', 'PRP'),\n",
              " (\"'ll\", 'MD'),\n",
              " ('flag', 'VB'),\n",
              " ('two', 'CD'),\n",
              " ('styles', 'NNS'),\n",
              " ('section', 'NN'),\n",
              " ('titles', 'NNS'),\n",
              " (',', ','),\n",
              " ('later', 'RB'),\n",
              " ('chapters', 'NNS'),\n",
              " ('mix', 'VBP'),\n",
              " ('styles', 'NNS'),\n",
              " ('without', 'IN'),\n",
              " ('up-front', 'JJ'),\n",
              " ('.', '.'),\n",
              " ('We', 'PRP'),\n",
              " ('hope', 'VBP'),\n",
              " ('style', 'NN'),\n",
              " ('introduction', 'NN'),\n",
              " ('gives', 'VBZ'),\n",
              " ('authentic', 'JJ'),\n",
              " ('taste', 'NN'),\n",
              " ('come', 'NN'),\n",
              " ('later', 'RB'),\n",
              " (',', ','),\n",
              " ('covering', 'VBG'),\n",
              " ('range', 'NN'),\n",
              " ('elementary', 'JJ'),\n",
              " ('concepts', 'NNS'),\n",
              " ('linguistics', 'NNS'),\n",
              " ('computer', 'NN'),\n",
              " ('science', 'NN'),\n",
              " ('.', '.'),\n",
              " ('If', 'IN'),\n",
              " ('basic', 'JJ'),\n",
              " ('familiarity', 'NN'),\n",
              " ('areas', 'NNS'),\n",
              " (',', ','),\n",
              " ('skip', 'VBD'),\n",
              " ('5', 'CD'),\n",
              " (';', ':'),\n",
              " ('repeat', 'NN'),\n",
              " ('important', 'JJ'),\n",
              " ('points', 'NNS'),\n",
              " ('later', 'JJ'),\n",
              " ('chapters', 'NNS'),\n",
              " (',', ','),\n",
              " ('miss', 'VBP'),\n",
              " ('anything', 'NN'),\n",
              " ('easily', 'RB'),\n",
              " ('consult', 'VBZ'),\n",
              " ('online', 'JJ'),\n",
              " ('reference', 'NN'),\n",
              " ('material', 'NN'),\n",
              " ('http', 'NN'),\n",
              " (':', ':'),\n",
              " ('//nltk.org/', 'NN'),\n",
              " ('.', '.'),\n",
              " ('If', 'IN'),\n",
              " ('material', 'JJ'),\n",
              " ('completely', 'RB'),\n",
              " ('new', 'JJ'),\n",
              " (',', ','),\n",
              " ('chapter', 'NN'),\n",
              " ('raise', 'NN'),\n",
              " ('questions', 'NNS'),\n",
              " ('answers', 'NNS'),\n",
              " (',', ','),\n",
              " ('questions', 'NNS'),\n",
              " ('addressed', 'VBD'),\n",
              " ('rest', 'JJ'),\n",
              " ('book', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M567QfQsdsqu"
      },
      "source": [
        "Have a look at  - <a href=\"http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/\"> List of POS Tags </a> . To know more about these words (NN, RB, VBZ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "bcS-xtOIdsqv"
      },
      "source": [
        "# Chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBb5MVDqdsqv"
      },
      "source": [
        "Chunking is basically identification of parts of speech and short phrases ( i.e noun phrase ) in a text. Parts of Speech gives us information about whether words are noun, adjective or verbs, but sometimes it is useful to have more information about the structure of a text. We use chunking in <a href=\"http://en.wikipedia.org/wiki/Named_entity_recognition\"> Named Entity Recognition </a> in which we are interested in finding named entity in the text.\n",
        "\n",
        "Chunking is useful for extracting meaningful information from a text. Chunking allows us to extract group of words with set characteristics. \n",
        "\n",
        "For ex - john was driving so fast.\n",
        "\n",
        "In the above example, named entity for john will be \"Person\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vinCOmqdsqv"
      },
      "source": [
        "from nltk.chunk import RegexpParser\n",
        "from nltk import sent_tokenize,word_tokenize"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgzCwch9dsqv"
      },
      "source": [
        "pattern = \"\"\"\n",
        "    NP: {<JJ>*<NN>+}\n",
        "    {<JJ>*<NN><CC>*<NN>+}\n",
        "    \"\"\"\n",
        "#pattern to detect noun phrases"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f8YEQBXdsqv"
      },
      "source": [
        "chunker = RegexpParser(pattern)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGzs9XJNdsqv"
      },
      "source": [
        "text = \"\"\"\n",
        "This chapter is divided into sections that skip between two quite different styles. In the \"computing with language\" sections we will take on some linguistically motivated programming tasks without necessarily explaining how they work. In the \"closer look at Python\" sections we will systematically review key programming concepts. We'll flag the two styles in the section titles, but later chapters will mix both styles without being so up-front about it. We hope this style of introduction gives you an authentic taste of what will come later, while covering a range of elementary concepts in linguistics and computer science. If you have basic familiarity with both areas, you can skip to 5; we will repeat any important points in later chapters, and if you miss anything you can easily consult the online reference material at http://nltk.org/. If the material is completely new to you, this chapter will raise more questions than it answers, questions that are addressed in the rest of this book.\"\"\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHHXbgJAdsqw"
      },
      "source": [
        "tokenized_sentence = nltk.sent_tokenize(text)  # Tokenize the text into sentences.\n",
        "tokenized_words = [nltk.word_tokenize(sentence) for sentence in tokenized_sentence]  # Tokenize words in sentences.\n",
        "tagged_words = [nltk.pos_tag(word) for word in tokenized_words]  # Tag words for POS in each sentence.\n",
        "word_tree = [chunker.parse(word) for word in tagged_words]  # Identify NP chunks"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGJh1vE4t088",
        "outputId": "be6a8ff6-ce9e-4d72-eb69-731391a25542"
      },
      "source": [
        "print(word_tree[0])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  This/DT\n",
            "  (NP chapter/NN)\n",
            "  is/VBZ\n",
            "  divided/VBN\n",
            "  into/IN\n",
            "  sections/NNS\n",
            "  that/WDT\n",
            "  skip/VBP\n",
            "  between/IN\n",
            "  two/CD\n",
            "  quite/RB\n",
            "  different/JJ\n",
            "  styles/NNS\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "gpUReoV2dsqw"
      },
      "source": [
        "# Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epx-O5nLdsqw"
      },
      "source": [
        "Named Entity Recognition is one of the most important task in information extraction. It basically means extracting named entities in a text. For example, if we have a text - \"Tirth Patel is a 3rd year CSE student at Nirma University\" then NER for Tirth will be Person, for Nirma University it will be Organization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8HBUA-Zdsqx",
        "outputId": "67423705-8f6e-430c-c600-8ba490750e78"
      },
      "source": [
        "from nltk import ne_chunk, pos_tag,  word_tokenize\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        " \n",
        "sentence = \"Tirth Patel is a 3rd year CSE student at Nirma University\"\n",
        " \n",
        "print(ne_chunk(pos_tag(word_tokenize(sentence))))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "(S\n",
            "  (PERSON Tirth/NNP)\n",
            "  (PERSON Patel/NNP)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  3rd/CD\n",
            "  year/NN\n",
            "  (ORGANIZATION CSE/NNP)\n",
            "  student/NN\n",
            "  at/IN\n",
            "  (ORGANIZATION Nirma/NNP University/NNP))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-u4-5B1dsqx"
      },
      "source": [
        "# Stemming and Lemmatizing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukw9UNL3dsqx"
      },
      "source": [
        "Stemming and Lemmatizing both really means removing morphological affixes from words, that is, leaving only the word stem. This is very helpful in many natural language processing tasks. Stem of a word \"running\" is \"run\", \"makes\" -> \"make\",etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03EH6t1adsqy"
      },
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YS7avRG0dsqy",
        "outputId": "6d0582d1-40ee-44f9-d031-ce09f15cd27c"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmer.stem(\"running\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'run'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uSBCAEyjdsqy",
        "outputId": "1785c5f0-a035-4a97-a17f-4d6bbbd823e3"
      },
      "source": [
        "stemmer.stem(\"having\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz-U8K28dsqy"
      },
      "source": [
        "#another stemmer is snowball\n",
        "from nltk.stem import SnowballStemmer"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "h3g5Hsh1dsqy"
      },
      "source": [
        "stemmer2 = SnowballStemmer(\"english\")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4nDsdT4qdsqz",
        "outputId": "a0a081f6-0e2d-4569-a301-8c622fb19aa0"
      },
      "source": [
        "stemmer2.stem(\"changing\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'chang'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTOFr_yKdsqz"
      },
      "source": [
        "Lemmatization is closely related to stemming. But lemmatization also takes into account the context in which word appears. Stemmers are just a set of rules, such as remove \"s\" from the end if xyz condition is satisfied. Stemmers are faster than lemmatizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6xwMlFmdsqz",
        "outputId": "238f1a8d-72b5-434c-e6cc-69b741d0fab2"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Fy515Oyhdsqz"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NqHPaMkdsqz",
        "outputId": "a52af55c-67e9-447e-ec44-fe7b99870bef"
      },
      "source": [
        "input_str=\"been had done languages cities mice\"\r\n",
        "input_str=word_tokenize(input_str)\r\n",
        "for word in input_str:\r\n",
        "    print(lemmatizer.lemmatize(word))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "been\n",
            "had\n",
            "done\n",
            "language\n",
            "city\n",
            "mouse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LunaaCl3dsq0"
      },
      "source": [
        "you can also use <a href=\"https://stanfordnlp.github.io/CoreNLP/\">stanford core nlp modules</a> such as NER tagger in nltk. Usually they are faster and more accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIs2yZE6dsq1"
      },
      "source": [
        "# Frequency Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWW8GR15dsq1"
      },
      "source": [
        "It is usually required to count the frequency of a word in given text. We can use FreqDist to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wizq73Rbdsq1"
      },
      "source": [
        "t = \"\"\"This chapter is divided into sections that skip between two quite different styles. In the \"computing with language\" sections we will take on some linguistically motivated programming tasks without necessarily explaining how they work. In the \"closer look at Python\" sections we will systematically review key programming concepts. We'll flag the two styles in the section titles, but later chapters will mix both styles without being so up-front about it. We hope this style of introduction gives you an authentic taste of what will come later, while covering a range of elementary concepts in linguistics and computer science. If you have basic familiarity with both areas, you can skip to 5; we will repeat any important points in later chapters, and if you miss anything you can easily consult the online reference material at http://nltk.org/. If the material is completely new to you, this chapter will raise more questions than it answers, questions that are addressed in the rest of this book.\"\"\""
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "crN4hqnLdsq2"
      },
      "source": [
        "from nltk.probability import FreqDist"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "A5usOcYKdsq2"
      },
      "source": [
        "fd = FreqDist(nltk.word_tokenize(t))"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biOIbZIidsq2",
        "outputId": "185445f0-b950-45af-990d-b15f17dc548c"
      },
      "source": [
        "fd"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({\"''\": 2,\n",
              "          \"'ll\": 1,\n",
              "          ',': 6,\n",
              "          '.': 7,\n",
              "          '//nltk.org/': 1,\n",
              "          '5': 1,\n",
              "          ':': 1,\n",
              "          ';': 1,\n",
              "          'If': 2,\n",
              "          'In': 2,\n",
              "          'Python': 1,\n",
              "          'This': 1,\n",
              "          'We': 2,\n",
              "          '``': 2,\n",
              "          'a': 1,\n",
              "          'about': 1,\n",
              "          'addressed': 1,\n",
              "          'an': 1,\n",
              "          'and': 2,\n",
              "          'answers': 1,\n",
              "          'any': 1,\n",
              "          'anything': 1,\n",
              "          'are': 1,\n",
              "          'areas': 1,\n",
              "          'at': 2,\n",
              "          'authentic': 1,\n",
              "          'basic': 1,\n",
              "          'being': 1,\n",
              "          'between': 1,\n",
              "          'book': 1,\n",
              "          'both': 2,\n",
              "          'but': 1,\n",
              "          'can': 2,\n",
              "          'chapter': 2,\n",
              "          'chapters': 2,\n",
              "          'closer': 1,\n",
              "          'come': 1,\n",
              "          'completely': 1,\n",
              "          'computer': 1,\n",
              "          'computing': 1,\n",
              "          'concepts': 2,\n",
              "          'consult': 1,\n",
              "          'covering': 1,\n",
              "          'different': 1,\n",
              "          'divided': 1,\n",
              "          'easily': 1,\n",
              "          'elementary': 1,\n",
              "          'explaining': 1,\n",
              "          'familiarity': 1,\n",
              "          'flag': 1,\n",
              "          'gives': 1,\n",
              "          'have': 1,\n",
              "          'hope': 1,\n",
              "          'how': 1,\n",
              "          'http': 1,\n",
              "          'if': 1,\n",
              "          'important': 1,\n",
              "          'in': 4,\n",
              "          'into': 1,\n",
              "          'introduction': 1,\n",
              "          'is': 2,\n",
              "          'it': 2,\n",
              "          'key': 1,\n",
              "          'language': 1,\n",
              "          'later': 3,\n",
              "          'linguistically': 1,\n",
              "          'linguistics': 1,\n",
              "          'look': 1,\n",
              "          'material': 2,\n",
              "          'miss': 1,\n",
              "          'mix': 1,\n",
              "          'more': 1,\n",
              "          'motivated': 1,\n",
              "          'necessarily': 1,\n",
              "          'new': 1,\n",
              "          'of': 4,\n",
              "          'on': 1,\n",
              "          'online': 1,\n",
              "          'points': 1,\n",
              "          'programming': 2,\n",
              "          'questions': 2,\n",
              "          'quite': 1,\n",
              "          'raise': 1,\n",
              "          'range': 1,\n",
              "          'reference': 1,\n",
              "          'repeat': 1,\n",
              "          'rest': 1,\n",
              "          'review': 1,\n",
              "          'science': 1,\n",
              "          'section': 1,\n",
              "          'sections': 3,\n",
              "          'skip': 2,\n",
              "          'so': 1,\n",
              "          'some': 1,\n",
              "          'style': 1,\n",
              "          'styles': 3,\n",
              "          'systematically': 1,\n",
              "          'take': 1,\n",
              "          'tasks': 1,\n",
              "          'taste': 1,\n",
              "          'than': 1,\n",
              "          'that': 2,\n",
              "          'the': 7,\n",
              "          'they': 1,\n",
              "          'this': 3,\n",
              "          'titles': 1,\n",
              "          'to': 2,\n",
              "          'two': 2,\n",
              "          'up-front': 1,\n",
              "          'we': 3,\n",
              "          'what': 1,\n",
              "          'while': 1,\n",
              "          'will': 6,\n",
              "          'with': 2,\n",
              "          'without': 2,\n",
              "          'work': 1,\n",
              "          'you': 6})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "glkL0U7edsq2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7d1mMHauLes"
      },
      "source": [
        "# Word Sense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muaNxgcUuLes"
      },
      "source": [
        "Word sense is one of the meaning of a word. There are some english words which has multiple meanings such as bench. NLTK provides an api for WordNet which is a semantic graph for words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgXA0YJFuLes",
        "outputId": "8e6718e1-1665-4012-ed88-71dd98cf1117"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "wn.synsets('man')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('man.n.01'),\n",
              " Synset('serviceman.n.01'),\n",
              " Synset('man.n.03'),\n",
              " Synset('homo.n.02'),\n",
              " Synset('man.n.05'),\n",
              " Synset('man.n.06'),\n",
              " Synset('valet.n.01'),\n",
              " Synset('man.n.08'),\n",
              " Synset('man.n.09'),\n",
              " Synset('man.n.10'),\n",
              " Synset('world.n.08'),\n",
              " Synset('man.v.01'),\n",
              " Synset('man.v.02')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWTwkuYXuLes",
        "outputId": "25658f30-d9a3-4b30-b52d-53eee7ad9b01"
      },
      "source": [
        "wn.synsets('man')[1].definition()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'someone who serves in the armed forces; a member of a military force'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa1zt-cruLet",
        "outputId": "c489a774-0838-47d5-ff92-7f0a76f31411"
      },
      "source": [
        "wn.synsets('dog')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('dog.n.01'),\n",
              " Synset('frump.n.01'),\n",
              " Synset('dog.n.03'),\n",
              " Synset('cad.n.01'),\n",
              " Synset('frank.n.02'),\n",
              " Synset('pawl.n.01'),\n",
              " Synset('andiron.n.01'),\n",
              " Synset('chase.v.01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axW3Oc_JuLet"
      },
      "source": [
        "dog = wn.synset('dog.n.01')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvnp1TaBuLet",
        "outputId": "cb87afc5-73bc-40c3-98af-7791e92593fa"
      },
      "source": [
        "dog.examples()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the dog barked all night'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SWwnFsruLet",
        "outputId": "16b12d1a-316b-4eb4-b2b7-5fb464570c5a"
      },
      "source": [
        "dog.hypernyms()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    }
  ]
}